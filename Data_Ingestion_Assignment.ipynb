{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0a4b69",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "We want to investigate different ways of reading in a large CSV file when using Python. The goal is to compare the computational efficiency of different methods. We will use three methods, which are listed below:\n",
    "\n",
    "* Pandas\n",
    "* Dask\n",
    "* Modin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d033e4",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "The dataset used here is an artificial version of the iris dataset. To artificially increase the size of the file, the original dataset was concatenated with itself for many iterations. Thus, the new CSV file, called \"big_iris\", consists of many copies of the original iris dataset joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86a6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
